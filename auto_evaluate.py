#!/usr/bin/env python3
"""
OpenAIX æ¯å°æ—¶è‡ªåŠ¨è¯„æµ‹è„šæœ¬ v2.0
- æ¯å°æ—¶è¯„æµ‹5ä¸ªç½‘ç«™
- å…¨å±€å»é‡ï¼Œä¸é‡å¤è¯„æµ‹
- ä¼˜åŒ–ç›®å½•ç»“æ„ï¼šæŒ‰æ—¥æœŸåˆ†ç›®å½•
- æ›´æ–° README å¹¶æ¨é€åˆ° GitHub
"""

import os
import sys
import json
import random
import subprocess
import requests
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: beautifulsoup4 æœªå®‰è£…ï¼Œç½‘ç«™å†…å®¹åˆ†æåŠŸèƒ½å°†å—é™")
    print("   å®‰è£…å‘½ä»¤: pip install beautifulsoup4 lxml")

# é…ç½®
PROJECT_DIR = Path("/home/wesley/.openclaw/workspace/openaix-core")
DATA_DIR = PROJECT_DIR / "data" / "evaluations"
URLS_FILE = PROJECT_DIR / "websites.txt"
BENCHMARK_SCRIPT = PROJECT_DIR / "benchmark.py"
VENV_PYTHON = PROJECT_DIR / "venv" / "bin" / "python3"
EVALUATED_LOG = DATA_DIR / "evaluated_sites.json"
BATCH_SIZE = 3  # æ¯å°æ—¶è¯„æµ‹æ•°é‡

# ç¡®ä¿ç›®å½•å­˜åœ¨
DATA_DIR.mkdir(parents=True, exist_ok=True)

# é»˜è®¤ç½‘ç«™åˆ—è¡¨
DEFAULT_WEBSITES = [
    "https://openai.com",
    "https://anthropic.com",
    "https://claude.ai",
    "https://gemini.google.com",
    "https://docs.python.org",
    "https://github.com",
    "https://stackoverflow.com",
    "https://medium.com",
    "https://apple.com",
    "https://google.com",
    "https://microsoft.com",
    "https://amazon.com",
    "https://shopify.com",
    "https://notion.so",
    "https://figma.com",
    "https://vercel.com",
    "https://nextjs.org",
    "https://react.dev",
    "https://vuejs.org",
    "https://tailwindcss.com",
    "https://stripe.com",
    "https://twilio.com",
    "https://developer.mozilla.org",
    "https://docs.npmjs.com",
    "https://angular.io",
    "https://docs.astro.build",
    "https://netlify.com",
    "https://cloudflare.com",
    "https://aws.amazon.com",
    "https://linear.app",
    "https://substack.com",
    "https://dev.to",
    "https://meta.com",
    "https://twitter.com",
    "https://nodejs.org",
    "https://redis.io",
    "https://postgresql.org",
    "https://mongodb.com",
]


def load_websites():
    """åŠ è½½ç½‘ç«™åˆ—è¡¨"""
    if URLS_FILE.exists():
        with open(URLS_FILE, 'r') as f:
            return [line.strip() for line in f if line.strip() and not line.startswith('#')]
    return DEFAULT_WEBSITES


def get_evaluated_sites():
    """è·å–æ‰€æœ‰å·²è¯„æµ‹è¿‡çš„ç½‘ç«™ï¼ˆå…¨å±€å»é‡ï¼‰"""
    evaluated = set()
    
    # ä»æ—¥å¿—æ–‡ä»¶è¯»å–
    if EVALUATED_LOG.exists():
        try:
            with open(EVALUATED_LOG, 'r') as f:
                data = json.load(f)
                evaluated.update(data.get('sites', []))
        except:
            pass
    
    # ä»ç°æœ‰JSONæ–‡ä»¶æ‰«æï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
    for json_file in DATA_DIR.rglob('*.json'):
        if json_file.name == 'evaluated_sites.json':
            continue
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
                if data.get('url'):
                    evaluated.add(data['url'])
        except:
            pass
    
    return evaluated


def save_evaluated_sites(sites):
    """ä¿å­˜å·²è¯„æµ‹ç½‘ç«™åˆ—è¡¨"""
    data = {
        'sites': list(sites),
        'last_updated': datetime.now().isoformat(),
        'total_count': len(sites)
    }
    with open(EVALUATED_LOG, 'w') as f:
        json.dump(data, f, indent=2)


def select_websites(batch_size=BATCH_SIZE):
    """é€‰æ‹©ä¸€æ‰¹æœªè¯„æµ‹çš„ç½‘ç«™"""
    websites = load_websites()
    evaluated = get_evaluated_sites()
    
    # è¿‡æ»¤æœªè¯„æµ‹çš„
    candidates = [url for url in websites if url not in evaluated]
    
    if len(candidates) < batch_size:
        print(f"âš ï¸ å‰©ä½™æœªè¯„æµ‹ç½‘ç«™ä»… {len(candidates)} ä¸ªï¼Œä¸è¶³ {batch_size} ä¸ª")
        print("   å°†ä»å…¨éƒ¨ç½‘ç«™ä¸­éšæœºé€‰æ‹©ï¼ˆä¼šé‡å¤è¯„æµ‹ï¼‰")
        candidates = websites
    
    # éšæœºé€‰æ‹© batch_size ä¸ª
    selected = random.sample(candidates, min(batch_size, len(candidates)))
    return selected


def analyze_website_content(url, timeout=10):
    """
    åˆ†æç½‘ç«™å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ç”¨äºæ•°æ®åº“å»ºè®¾
    
    è¿”å›:
        dict: åŒ…å«ç½‘ç«™å®šä½ã€ä¸»è¦å†…å®¹ã€AIå¯ç”¨ä¿¡æ¯ç­‰
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        
        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        response.raise_for_status()
        
        content = response.text
        parsed_url = urlparse(response.url)
        
        analysis = {
            'url': url,
            'final_url': response.url,
            'domain': parsed_url.netloc,
            'title': '',
            'meta_description': '',
            'main_content_preview': '',
            'content_type': 'unknown',
            'ai_use_cases': [],
            'key_topics': [],
            'language': '',
            'has_structured_data': False,
            'extracted_at': datetime.now().isoformat()
        }
        
        if BS4_AVAILABLE and content:
            soup = BeautifulSoup(content, 'lxml')
            
            # æå–æ ‡é¢˜
            title_tag = soup.find('title')
            if title_tag:
                analysis['title'] = title_tag.get_text(strip=True)[:200]
            
            # æå– meta description
            meta_desc = soup.find('meta', attrs={'name': 'description'}) or \
                       soup.find('meta', attrs={'property': 'og:description'})
            if meta_desc:
                analysis['meta_description'] = meta_desc.get('content', '')[:500]
            
            # æå–è¯­è¨€
            html_tag = soup.find('html')
            if html_tag and html_tag.get('lang'):
                analysis['language'] = html_tag.get('lang')
            
            # æå–ä¸»è¦å†…å®¹é¢„è§ˆ
            content_selectors = ['main', 'article', '[role="main"]', '.content', 
                               '.main-content', '#content', '#main-content', 'body']
            
            main_text = ''
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(separator=' ', strip=True)
                    text = ' '.join(text.split())
                    main_text = text[:1500]  # å–å‰1500å­—ç¬¦
                    break
            
            analysis['main_content_preview'] = main_text
            
            # æå–å…³é”®ä¸»é¢˜ï¼ˆä» headings ä¸­ï¼‰
            headings = soup.find_all(['h1', 'h2', 'h3'])
            keywords = []
            for h in headings[:10]:
                text = h.get_text(strip=True)
                if text and len(text) > 3:
                    keywords.append(text[:100])
            analysis['key_topics'] = keywords[:5]
            
            # æ£€æµ‹ç»“æ„åŒ–æ•°æ®
            structured_selectors = ['script[type="application/ld+json"]', '[itemscope]']
            for selector in structured_selectors:
                if soup.select(selector):
                    analysis['has_structured_data'] = True
                    break
            
            # æ£€æµ‹å†…å®¹ç±»å‹
            domain_lower = analysis['domain'].lower()
            desc_lower = analysis['meta_description'].lower()
            
            if any(x in domain_lower for x in ['docs.', 'documentation', 'wiki', 'help']) or \
               'documentation' in desc_lower:
                analysis['content_type'] = 'documentation'
            elif any(x in domain_lower for x in ['news', 'blog', 'medium', 'substack']):
                analysis['content_type'] = 'news/blog'
            elif any(x in domain_lower for x in ['shop', 'store', 'amazon', 'ebay']) or \
                 soup.find('meta', attrs={'property': 'product:price'}):
                analysis['content_type'] = 'e-commerce'
            elif any(x in domain_lower for x in ['edu', 'university', 'college', 'mit.', 'harvard.']):
                analysis['content_type'] = 'education'
            elif any(x in domain_lower for x in ['github', 'gitlab', 'bitbucket']):
                analysis['content_type'] = 'code_repository'
            elif any(x in domain_lower for x in ['youtube', 'vimeo', 'tiktok', 'bilibili']):
                analysis['content_type'] = 'video_platform'
            elif soup.find('form'):
                analysis['content_type'] = 'web_application'
            else:
                analysis['content_type'] = 'general_website'
            
            # AI ä½¿ç”¨åœºæ™¯å»ºè®®
            ai_use_cases_map = {
                'documentation': [
                    'æŠ€æœ¯æ–‡æ¡£é—®ç­”ä¸æ£€ç´¢',
                    'API ä½¿ç”¨ç¤ºä¾‹ç”Ÿæˆ',
                    'é”™è¯¯æ’æŸ¥ä¸è§£å†³æ–¹æ¡ˆæ¨è',
                    'ä»£ç ç‰‡æ®µæå–ä¸è§£é‡Š',
                    'ç‰ˆæœ¬å˜æ›´è¯´æ˜åˆ†æ'
                ],
                'news/blog': [
                    'å†…å®¹æ‘˜è¦ä¸å…³é”®ä¿¡æ¯æå–',
                    'è¡Œä¸šè¶‹åŠ¿åˆ†æä¸é¢„æµ‹',
                    'å¤šè¯­è¨€ç¿»è¯‘ä¸æœ¬åœ°åŒ–',
                    'æƒ…æ„Ÿåˆ†æä¸è§‚ç‚¹è¯†åˆ«',
                    'çƒ­ç‚¹è¯é¢˜è¿½è¸ª'
                ],
                'e-commerce': [
                    'äº§å“ä»·æ ¼ç›‘æ§ä¸æ¯”è¾ƒ',
                    'å•†å“æè¿°æ™ºèƒ½ä¼˜åŒ–',
                    'ç”¨æˆ·è¯„ä»·æƒ…æ„Ÿåˆ†æ',
                    'åº“å­˜çŠ¶æ€ç›‘æ§',
                    'ç«å“åˆ†ææŠ¥å‘Šç”Ÿæˆ'
                ],
                'education': [
                    'å­¦ä¹ èµ„æ–™æ™ºèƒ½æ•´ç†',
                    'è¯¾ç¨‹æ¨èä¸è§„åˆ’',
                    'ç ”ç©¶è®ºæ–‡æ‘˜è¦ä¸åˆ†æ',
                    'çŸ¥è¯†ç‚¹æå–ä¸çŸ¥è¯†å›¾è°±æ„å»º',
                    'å­¦æœ¯èµ„æºæ£€ç´¢'
                ],
                'code_repository': [
                    'ä»£ç å®¡æŸ¥ä¸è´¨é‡åˆ†æ',
                    'é¡¹ç›®æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ',
                    'ä¾èµ–å…³ç³»ä¸å®‰å…¨åˆ†æ',
                    'åŠŸèƒ½æ¨¡å—è¯†åˆ«ä¸æå–',
                    'è´¡çŒ®è€…è¡Œä¸ºåˆ†æ'
                ],
                'video_platform': [
                    'è§†é¢‘å†…å®¹è½¬å½•ä¸æ‘˜è¦',
                    'å­—å¹•ç”Ÿæˆä¸ç¿»è¯‘',
                    'å†…å®¹åˆ†ç±»ä¸æ ‡ç­¾æå–',
                    'åˆ›ä½œè€…åˆ†æ',
                    'è¶‹åŠ¿è§†é¢‘è¯†åˆ«'
                ],
                'web_application': [
                    'åŠŸèƒ½å¯ç”¨æ€§ç›‘æ§',
                    'ç”¨æˆ·æµç¨‹åˆ†æ',
                    'è¡¨å•æ•°æ®å¤„ç†',
                    'è‡ªåŠ¨åŒ–æµ‹è¯•æ”¯æŒ',
                    'æ€§èƒ½ç›‘æ§'
                ],
                'general_website': [
                    'ç½‘ç«™å†…å®¹æ‘˜è¦',
                    'ä¿¡æ¯åˆ†ç±»ä¸æ ‡ç­¾',
                    'å…³é”®è¯ä¸ä¸»é¢˜æå–',
                    'æ›´æ–°ç›‘æ§ä¸å˜æ›´æ£€æµ‹',
                    'SEO å†…å®¹åˆ†æ'
                ]
            }
            
            analysis['ai_use_cases'] = ai_use_cases_map.get(analysis['content_type'], 
                                                             ai_use_cases_map['general_website'])
            
            # ç”Ÿæˆç½‘ç«™å®šä½æè¿°
            if analysis['title'] and analysis['meta_description']:
                analysis['site_positioning'] = f"{analysis['title']} - {analysis['meta_description'][:200]}"
            elif analysis['title']:
                analysis['site_positioning'] = analysis['title']
            else:
                analysis['site_positioning'] = f"{analysis['domain']} ({analysis['content_type']})"
        
        return analysis
        
    except requests.exceptions.Timeout:
        return {'error': 'è¿æ¥è¶…æ—¶', 'url': url, 'content_type': 'timeout', 'extracted_at': datetime.now().isoformat()}
    except requests.exceptions.RequestException as e:
        return {'error': str(e), 'url': url, 'content_type': 'error', 'extracted_at': datetime.now().isoformat()}
    except Exception as e:
        return {'error': f'åˆ†æé”™è¯¯: {str(e)}', 'url': url, 'content_type': 'error', 'extracted_at': datetime.now().isoformat()}


def evaluate_website(url, output_dir):
    """è¯„æµ‹å•ä¸ªç½‘ç«™ï¼ˆåŒ…å«AIXè¯„åˆ†å’Œå†…å®¹åˆ†æï¼‰"""
    timestamp = datetime.now().strftime('%H%M%S')
    domain = url.replace('https://', '').replace('http://', '').replace('/', '_')
    output_file = output_dir / f"{timestamp}_{domain}.json"
    report_file = output_dir / f"{timestamp}_{domain}.md"
    
    print(f"\nğŸ” è¯„æµ‹: {url}")
    
    # Step 1: è¿è¡Œ AIX è¯„åˆ†
    cmd = [
        str(VENV_PYTHON),
        str(BENCHMARK_SCRIPT),
        url,
        "--output", str(report_file),
        "--json",
        "--timeout", "15"
    ]
    
    aix_result = None
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        if result.returncode == 0:
            json_file = str(report_file).replace('.md', '.json')
            if os.path.exists(json_file):
                with open(json_file, 'r') as f:
                    data = json.load(f)
                aix_result = data[0] if data else None
    except Exception as e:
        print(f"   âš ï¸ AIXè¯„åˆ†é”™è¯¯: {e}")
    
    # Step 2: åˆ†æç½‘ç«™å†…å®¹
    print(f"   ğŸ“„ åˆ†æç½‘ç«™å†…å®¹...")
    content_analysis = analyze_website_content(url, timeout=8)
    
    # Step 3: åˆå¹¶ç»“æœ
    evaluation = {
        "url": url,
        "timestamp": datetime.now().isoformat(),
        "result": aix_result,
        "content_analysis": content_analysis
    }
    
    # ä¿å­˜ç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation, f, indent=2, ensure_ascii=False)
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    if aix_result and aix_result.get("success"):
        score = aix_result["result"]["score"]
        grade = aix_result["result"]["grade"]
        content_type = content_analysis.get('content_type', 'unknown')
        print(f"   âœ… AIX: {score}/100 ({grade}) | ç±»å‹: {content_type}")
        return evaluation
    elif content_analysis and not content_analysis.get('error'):
        content_type = content_analysis.get('content_type', 'unknown')
        print(f"   âš ï¸ AIXå¤±è´¥ï¼Œå†…å®¹åˆ†æå®Œæˆ | ç±»å‹: {content_type}")
        return evaluation
    else:
        print(f"   âŒ å¤±è´¥")
        return None


def update_readme(evaluations):
    """æ›´æ–° README å±•ç¤ºæœ€æ–°è¯„æµ‹ç»“æœ"""
    # è¯»å–å†å²æ‰€æœ‰è¯„æµ‹
    all_evals = []
    for json_file in DATA_DIR.rglob('*.json'):
        if json_file.name in ['evaluated_sites.json']:
            continue
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
                if data.get("result") and data["result"].get("success"):
                    all_evals.append(data)
        except:
            pass
    
    # æŒ‰URLå»é‡ï¼Œä¿ç•™æœ€æ–°çš„
    seen_urls = {}
    for eval in sorted(all_evals, key=lambda x: x.get('timestamp', ''), reverse=True):
        url = eval['url']
        if url not in seen_urls:
            seen_urls[url] = eval
    
    unique_evals = list(seen_urls.values())
    
    if not unique_evals:
        print("âš ï¸ æ²¡æœ‰è¯„æµ‹æ•°æ®")
        return
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_sites = len(unique_evals)
    scores = [e['result']['result']['score'] for e in unique_evals]
    avg_score = sum(scores) / len(scores)
    
    # åˆ†çº§ç»Ÿè®¡
    grade_counts = {'S': 0, 'A': 0, 'B': 0, 'C': 0}
    for e in unique_evals:
        grade = e['result']['result']['grade']
        grade_letter = grade[6] if len(grade) > 6 else 'C'
        if grade_letter in grade_counts:
            grade_counts[grade_letter] += 1
    
    # ç”ŸæˆæŠ¥å‘Š
    lines = []
    lines.append("## ğŸ“Š è¯„æµ‹ç»Ÿè®¡\n")
    lines.append(f"- **æ€»è®¡è¯„æµ‹ç½‘ç«™**: {total_sites} ä¸ª")
    lines.append(f"- **å¹³å‡åˆ†æ•°**: {avg_score:.1f}/100")
    lines.append(f"- **æœ€åæ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
    
    lines.append("### ç­‰çº§åˆ†å¸ƒ\n")
    for grade in ['S', 'A', 'B', 'C']:
        count = grade_counts.get(grade, 0)
        bar = 'â–ˆ' * count
        lines.append(f"- **{grade}**: {count} {bar}")
    lines.append("")
    
    lines.append("### æœ€æ–°è¯„æµ‹ç»“æœ\n")
    lines.append("| ç½‘ç«™ | åˆ†æ•° | ç­‰çº§ | è¯„æµ‹æ—¶é—´ |")
    lines.append("|------|------|------|----------|")
    
    for eval in unique_evals[:15]:
        url = eval["url"].replace("https://", "").replace("http://", "")[:28]
        result = eval["result"]["result"]
        score = result["score"]
        grade = result["grade"][6] if len(result["grade"]) > 6 else "?"
        time = eval["timestamp"][:16].replace("T", " ") if 'T' in eval["timestamp"] else eval["timestamp"][:16]
        lines.append(f"| {url} | {score} | {grade} | {time} |")
    
    lines.append("")
    
    readme_section = "\n".join(lines)
    
    # æ›´æ–° README
    readme_path = PROJECT_DIR / "README.md"
    with open(readme_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    marker_start = "<!-- EVALUATION_RESULTS_START -->"
    marker_end = "<!-- EVALUATION_RESULTS_END -->"
    
    if marker_start in content and marker_end in content:
        parts = content.split(marker_start)
        new_content = parts[0] + marker_start + "\n\n" + readme_section + marker_end + content.split(marker_end)[1]
    else:
        new_content = content + "\n\n" + marker_start + "\n\n" + readme_section + marker_end + "\n"
    
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    print("âœ… README å·²æ›´æ–°")


def git_push():
    """æ¨é€åˆ° GitHub"""
    try:
        os.chdir(PROJECT_DIR)
        
        subprocess.run(["git", "config", "user.email", "auto@openaix.org"], check=False, capture_output=True)
        subprocess.run(["git", "config", "user.name", "OpenAIX Bot"], check=False, capture_output=True)
        
        subprocess.run(["git", "add", "-A"], check=False, capture_output=True)
        
        result = subprocess.run(["git", "diff", "--cached", "--quiet"], capture_output=True)
        if result.returncode == 0:
            print("â„¹ï¸ æ— æ›´æ”¹éœ€æäº¤")
            return
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        count = len(list(DATA_DIR.rglob('*.json'))) - 1  # æ’é™¤ evaluated_sites.json
        
        subprocess.run([
            "git", "commit", "-m", 
            f"ğŸ“Š Auto eval: {count} sites - {timestamp}"
        ], check=False, capture_output=True)
        
        # å…ˆæ‹‰å–å†æ¨é€ï¼Œé¿å…å†²çª
        pull_result = subprocess.run(["git", "pull", "--rebase"], capture_output=True, text=True, timeout=30)
        if pull_result.returncode != 0:
            print(f"   âš ï¸ æ‹‰å–å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶æ¨é€: {pull_result.stderr[:100]}")
        
        push_result = subprocess.run(["git", "push"], capture_output=True, text=True)
        if push_result.returncode == 0:
            print("âœ… å·²æ¨é€åˆ° GitHub")
        else:
            print(f"âš ï¸ æ¨é€é—®é¢˜: {push_result.stderr[:200]}")
            
    except Exception as e:
        print(f"âŒ Git é”™è¯¯: {e}")


def git_pull():
    """æ‹‰å–æœ€æ–°ä»£ç """
    try:
        os.chdir(PROJECT_DIR)
        print("ğŸ”„ æ‹‰å–æœ€æ–°ä»£ç ...")
        result = subprocess.run(
            ["git", "pull", "--rebase"],
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            print("   âœ… ä»£ç å·²æ›´æ–°")
        else:
            print(f"   âš ï¸ æ‹‰å–å¯èƒ½æœ‰å†²çª: {result.stderr[:100]}")
    except Exception as e:
        print(f"   âš ï¸ æ‹‰å–å¤±è´¥: {e}")


def main():
    print("="*60)
    print("ğŸ¤– OpenAIX è‡ªåŠ¨è¯„æµ‹ç³»ç»Ÿ v2.0")
    print("="*60)
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“¦ æ¯å°æ—¶è¯„æµ‹: {BATCH_SIZE} ä¸ªç½‘ç«™")
    print()
    
    # å…ˆæ‹‰å–æœ€æ–°ä»£ç 
    git_pull()
    print()
    
    # å·²è¯„æµ‹ç½‘ç«™
    evaluated = get_evaluated_sites()
    print(f"ğŸ“Š å·²è¯„æµ‹ç½‘ç«™: {len(evaluated)} ä¸ª")
    
    # é€‰æ‹©ç½‘ç«™
    urls = select_websites(BATCH_SIZE)
    print(f"ğŸ¯ æœ¬æ¬¡è¯„æµ‹: {len(urls)} ä¸ªç½‘ç«™")
    for url in urls:
        print(f"   â€¢ {url}")
    print()
    
    # åˆ›å»ºä»Šæ—¥ç›®å½•
    today = datetime.now().strftime('%Y%m%d')
    today_dir = DATA_DIR / today
    today_dir.mkdir(exist_ok=True)
    
    # è¯„æµ‹
    results = []
    new_evaluated = set()
    
    for url in urls:
        result = evaluate_website(url, today_dir)
        if result:
            results.append(result)
            new_evaluated.add(url)
    
    # æ›´æ–°å·²è¯„æµ‹åˆ—è¡¨
    evaluated.update(new_evaluated)
    save_evaluated_sites(evaluated)
    
    print(f"\nğŸ“ˆ æœ¬æ¬¡æˆåŠŸ: {len(results)}/{len(urls)}")
    
    if results:
        print("\nğŸ“ æ›´æ–° README...")
        update_readme(results)
        
        print("\nğŸš€ æ¨é€åˆ° GitHub...")
        git_push()
        
        print("\n" + "="*60)
        print("âœ… å®Œæˆ!")
        print("="*60)
    else:
        print("\n" + "="*60)
        print("âš ï¸ å…¨éƒ¨å¤±è´¥")
        print("="*60)
        sys.exit(1)


if __name__ == '__main__':
    main()
